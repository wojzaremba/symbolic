\section{Admissible Computation as a Grammar}\label{sec:grammars}

Our goal is to find a fast way to compute the target expression in
language and architecture of our choice.  We will achieve it by
developing literals in a grammar, up to degree of target
expression. Next, we will look for the linear combination of obtained
expressions to find how to express target expression exactly. If there
are multiple literals with the same expression (up to multiplicative real
constant), we choose one with the shortest computation time $t_\mathcal{C}$.

Let's ground our approach in an example. We assume that we are interest in
finding algorithm with smallest operation complexity, and computation platform
is a Matlab.  We consider following set of operations on matrices: 

\begin{figure}
\begin{framed}
\begin{align*}
&\text{{\bf Element wise multiplication}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}), (\mathbb{B} \in \mathcal{P}^{n \times m}_\beta, \mathcal{B}, t_\mathcal{B}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_{\alpha + \beta}, \mathcal{A} .* \mathcal{B}, O(t_\mathcal{A} + t_\mathcal{B} + nm)) \\
&\mathbb{C}_{i,j} = \mathbb{A}_{i,j}\mathbb{B}_{i, j} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\} \\
&\text{{\bf Matrix multiplication}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}), (\mathbb{B} \in \mathcal{P}^{m \times k}_\beta, \mathcal{B}, t_\mathcal{B}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times k}_{\alpha + \beta}, \mathcal{A} * \mathcal{B}, O(t_\mathcal{A} + t_\mathcal{B} + nmk)) \\
&\mathbb{C}_{i,k} = \sum_{k = 1}^m \mathbb{A}_{i,k}b_{k, j} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\} \\
&\text{{\bf Columns marginalization}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times 1}_\alpha, sum(\mathcal{A}, 2), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, 1} = \sum_{j = 1}^m \mathbb{A}_{i, j} \text{ for } i \in \{1, \dots, n\}\\
&\text{{\bf Rows marginalization}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{1 \times m}_\alpha, sum(\mathcal{A}, 1), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{1, j} = \sum_{i = 1}^m \mathbb{A}_{i, j} \text{ for } j \in \{1, \dots, m\}\\
&\text{{\bf Columns repetition}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times 1}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_\alpha, repmat(\mathcal{A}, 1, m), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, j} = \mathbb{A}_{i, 1} \text{ for } i \in \{1, \dots, n\}\\
&\text{{\bf Rows repetition}}\\
&(\mathbb{A} \in \mathcal{P}^{1 \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_\alpha, repmat(\mathcal{A}, n, 1), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, j} = \mathbb{A}_{1, j} \text{ for } j \in \{1, \dots, m\}\\
&\text{{\bf Entry repetition}}\\
&(\mathbb{A} \in \mathcal{P}^{1 \times 1}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_\alpha, repmat(\mathcal{A}, n, m), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, j} = \mathbb{A}_{1, 1} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\}\\
&\text{{\bf Transposition}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{m \times n}_\alpha, A', O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{j, i} = \mathbb{A}_{i, j} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\} \\
\end{align*}
\caption{Set of rules for attributive grammar. There is always finite number of literals up to degree $\alpha$.}
\end{framed}
\label{rules}
\end{figure}


To find any (or the cheapest) way of computing target expression $\mathbb{T}$ of degree $k$, we proceed as follows: 
\begin{itemize}
\item develop grammar to obtain all possible literals up to degree $k$. It gives rise to finite number of literals. 
\item if there are multiple way of computing the same expression up to multiplicative constant, choose one with shortest time.
\item find linear combination of expressions which is equal the target
  expression $\mathbb{T}$ (this relation is a symbolic relation, which
  is valid for any assignment of symbols). While solving this linear
  set of systems, choose solution with smallest sum of computation
  times. See \ref{sec:linear} for more details.
\item apply optimization like subexpression elimination, and exploit distributive property of multiplication in order to decrees final computational time of $\mathbb{T}$. This step is optional, and it won't decrease computational complexity, but it can improve performance.
\end{itemize}

Pseudo code for the algorithm is shown in Algorithm \ref{pseudocode}.

\begin{algorithm}
\caption{Find computation for expression}
\begin{algorithmic} 
\REQUIRE Target expression $\mathbb{T}$, initial expression $W$, $Rules$ = \{$R_1, \dots, R_n$\}, maximum degree of polynomials $k$
\ENSURE Computation $\mathcal{C}$ of expression $\mathbb{T}$.
\STATE Initialize set $\mathbb{S}$ of all admissible literals with $(W, \mathcal{W}, t_W)$
\WHILE{$\mathbb{S}$ grows}
\FORALL{literal $L \in \mathbb{S}$}
\FORALL{rule $R \in Rules$}
\STATE $L' \gets$ Apply rule $R$ to $L$
\IF {degree($L'$) $>$ $k$}
  \STATE \textbf{continue}
\ENDIF
\STATE \emph{// If expression not yet in $\mathbb{S}$ or it can be}
\STATE \emph{// computed faster add it.}
\IF {$L'.expr \not\in \mathbb{S}$ \textbf{or} $L'.time < S[L'].time$}
  \STATE Add $L'$ to $\mathbb{S}$
\ENDIF
\ENDFOR
\ENDFOR
\ENDWHILE
\STATE Find linear combination of expressions from literals stored in $\mathbb{S}$ to express $\mathbb{T}$
\STATE Run optimizer (optional)
\end{algorithmic}
\label{pseudocode}
\end{algorithm}


\subsection{Linear combination}
\label{sec:linear}
We look for a linear combination of generated expressions in our
literals, which is equal to the target expression. There are several
possible scenarios. First, there might be a \emph{single solution}, in
which case our linear solver will have a unique solution. In practice
however, this is uncommon. Second, there may be \emph{no solution} at
all.  In this case, our linear solver will indicate it, meaning that
the target expression is out of scope for the computation defined by
our grammar. Finally, there might be \emph{multiple solutions}. We
then look for the linear combination with the smallest cost
(computational time).  This is essentially an integer programing
problem, or knapsack problem, which are NP-complete. However, we can
relax this to a linear program, which will give a solution almost as
efficient as the optimal one.  Experiments suggest that is good to
also minimize number of coefficients, so avoiding unnecessary
non-integer coefficient values, which can contribute numerical errors.


%it is enough for us to get approximated
%solution (Rob: what kind of approximation), which can be easily obtained with linear solver.

It is worth noting that while it is easy to find best solution in
terms of {\em complexity}, it is NP-complete to find the best solution
in terms of {\em performance}. In order to find best solution in terms
of complexity, one has to run linear solver multiple times. Every
time, it should include broader number of literals. Starting with set
of literals having the smallest complexity (e.g.~those which are
$O(1)$), and adding new literals with higher and higher complexity
($O(n)$ and then $O(n^2)$ and so on). If algorithm, find solution at
some point, than this solution is optimal in terms of
complexity. However, discovery of the fastest performance (i.e.~lowest
runtime) requires exploration of all possible combinations of literals,
which is far more expensive.  

\subsection{Computation derivation as the formal system}

For Joan.

\subsection{Representation of Expressions}
%Rob: I did not understand this entire section.
Expressions are defined formal to belong to $\mathcal{P}^{n \times m}_\alpha$. There are various
ways how matrices of polynomials can be represented in the code software. Such representation is crucial, and has consequences in
 correctness of solution (due to numerical errors), and computation speed.

We will demonstrate different representations by considering two matrices of polynomials:
\begin{align*}
\mathbb{A} &= \begin{pmatrix} a^2 + 2ab + b^2 & a^2\\ b^2 & ab \end{pmatrix}, \mathbb{B} &= \begin{pmatrix} a + b & a\\ b & 2a + 2b \end{pmatrix}
\end{align*}


\subsubsection{Symbolic representation}
Expressions are defined as matrices of polynomials. The most straight forward 
representation is to store for every expression, its coefficients and powers of the every monomial.
This representation is exact, and corresponds one-to-one with our description of operations
defined by grammar. However, it has computational drawback. 
For instance, let's assume that we would like to multiply two polynomials, each having $1000$ monomials.
Such multiplication would take roughly $10^6$ operations. While the same operation on instantiations of
this polynomials would cost just a one multiplication.


We have implemented expressions in such representation, and it allowed us
to find patterns for polynomials up to degree $4$ (above degree $4$ it was consuming too much time).

This solution guarantees correctness, and it is easiest to debug.

Entry $(0, 0)$ of matrix $\mathbb{A}$ would be repressed as list of coefficients $[1, 2, 1]$, and list
of powers $\begin{pmatrix} 2 & 1 & 0\\ 0 & 1 & 2 \end{pmatrix}$. Entry $(0, 0)$ of matrix $\mathbb{B}$ would
be represented as list of coefficients $[1, 1]$, and list of powers 
$\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$. Result of element-wise multiplication of such expression,
would result in expression having in entry $(0, 0)$ value with list of coefficients $[1, 3, 3, 1]$, and
list of powers $\begin{pmatrix} 3 & 2 & 1 & 0 \\ 0 & 1 & 2 & 3 \end{pmatrix}$. 


\subsubsection{Evaluation of polynomials in $\mathbb{R}$}
Polynomials can be encoded 
by its evaluation at sufficiently many points (more than a degree of polynomial).
This encoding simplifies operations on expressions, and multiplication of expressions
in such representation is proportional to the number of evaluation points (for every evaluation
point, we have to perform just a common multiplication).

Actually, we need evaluate our polynomials in number of points which is larger 
than final size of linear equations \ref{sec:linear}. We do not have to recover coefficients of polynomials,
as we are just interest in finding proper linear combination of expressions.


This methods is numerically unstable
, and for polynomials of degree $k \geq 3$, it is not able to produce solutions, even if there are some.

Nice part about this representation is that operations like element-wise matrix multiplication, addition of matrices, etc.
correspond just to the same operations in our representation (of same computational cost multiplied by number of evaluation points). i.e. 
For element-wise multiplication of matrices of expressions, we
have to perform element-wise multiplication of real numbers (on multiple instances of random values assignments).

Let's assume that we evaluate polynomials in points $a = 0.5, b = 1.5$, and $a = 2.5, b = 3.5$
Then $\mathbb{A}, \mathbb{B}$ have the following evaluation:
\begin{align*}
\mathbb{A} &= \begin{pmatrix} 4 & 0.25\\ 2.25 & 0.75 \end{pmatrix}, \begin{pmatrix} 36 & 6.25\\ 12.25 & 8.75 \end{pmatrix} \\
\mathbb{B} &= \begin{pmatrix} 2 & 0.5\\ 1.5 & 4 \end{pmatrix}, \begin{pmatrix} 6 & 2.5\\ 3.5 & 12 \end{pmatrix}
\end{align*}
Element-wise multiplication of expressions $\mathbb{A}$, and $\mathbb{B}$ is just element-wise multiplication of evaluations
in the same points:
\begin{align*}
\begin{pmatrix} 8 & 0.125\\ 3.375 & 4 \end{pmatrix}, \begin{pmatrix} 216 & 15.625\\ 42.875 & 105 \end{pmatrix}
\end{align*}

\subsubsection{Evaluation of polynomials in $\mathbb{Z}_p$}
We use similar strategy as in case of looking into evaluations of polynomials in $\mathbb{R}$. However, in order
to avoid numerical errors, we replace real number computation with computation in $\mathbb{Z}_p$ group (for large prime number $p$). This
guarantees that all the results of the computation are represented correctly (there is no round off errors on integers). Nonetheless, there are few potential
issues. We could have two different expressions being represented the same way (conflict of representations). In the
real setting, we haven't run into this problem. Moreover, all the final coefficients after applying linear solver are
accurate up to $p$ offset. This turns out also not to be an issue. For all coefficients above $\frac{p}{2}$, we subtract $p$. 
This way all final coefficients are in the range $[-\frac{p}{2}, \frac{p}{2}]$. We have verified that such solutions are correct
(it can be considered as prior on magnitude of coefficients). 

This solution is fastest, and most reliable. However, it is difficult to debug. With such representation, we were able to
develop completely grammars up to degree $5$.


Let's assume that we evaluate polynomials in points $a = 1, b = 2$, and $a = 3, b = 4$, and we work in $Z_{11}$.
Then $\mathbb{A}, \mathbb{B}$ have the following evaluation:
\begin{align*}
\mathbb{A} &= \begin{pmatrix} 9 & 1\\ 4 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 9\\ 5 & 1 \end{pmatrix} \\
\mathbb{B} &= \begin{pmatrix} 3 & 1\\ 2 & 6 \end{pmatrix}, \begin{pmatrix} 7 & 3\\ 4 & 3 \end{pmatrix}
\end{align*}
Element-wise multiplication of expressions $\mathbb{A}$, and $\mathbb{B}$ is just element-wise multiplication of evaluations
in the same points modulo $11$:
\begin{align*}
\begin{pmatrix} 5 & 1\\ 8 & 1 \end{pmatrix}, \begin{pmatrix} 2 & 5\\ 9 & 3 \end{pmatrix}
\end{align*}

