\section{Admissible Computation as a Grammar}\label{sec:grammars}

Our goal is to find a fast way to compute the target expression in
a language and architecture of our choice. We will achieve it by
developing expressions in a grammar, up to degree of target
expression. Next, we will look for a linear combination of obtained
expressions to represent the target expression exactly. If the target expression
can be obtained in several ways, we choose the one with the shortest computation time.

Let's ground our approach in an example. We assume that we are interested in
finding algorithm with smallest operation complexity, and computation platform
is a Matlab. We consider following set of operations on matrices (Figures \ref{fig:rules2} and \ref{fig:rules}).


\begin{figure}
\begin{framed}
\begin{align*}
&\text{{\bf Element wise multiplication}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathbb{B} \in \mathcal{P}^{n \times m}_\beta \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_{\alpha + \beta} \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{B}.time + \mathbb{A}.n * \mathbb{B}.m);\\
&\mathbb{C}.computation := \mathbb{A}.computation .* \mathbb{B}.computation;\\
&\forall_{\substack{i \leq n\\j \leq m}}\text{ }\mathbb{C}.term[i][j] := \mathbb{A}.term[i][j] * \mathbb{B}.term[i][j];\\
%&\forall_{\substack{1 \leq i \leq n\\1 \leq j \leq m}}\text{ }\mathbb{C}.term[i][j] := \mathbb{A}.term[i][j] * \mathbb{B}.term[i][j];\\
& \\
&\text{{\bf Matrix multiplication}}\\
&\mathbb{A} \in \mathcal{P}^{n \times k}_\alpha, \mathbb{B} \in \mathcal{P}^{k \times m}_\beta \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_{\alpha + \beta} \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{B}.time + \mathbb{A}.n * \mathbb{A}.m * \mathbb{B}.m);\\
&\mathbb{C}.computation := \mathbb{A}.computation * \mathbb{B}.computation;\\
&\forall_{\substack{i \leq n\\j \leq m}}\mathbb{C}.term[i][j] := \sum_{k = 1}^m \mathbb{A}.term[i][k] * \mathbb{B}.term[k][j];
%&\forall_{\substack{1 \leq i \leq n\\1 \leq j \leq m}}\mathbb{C}.term[i][j] := \sum_{k = 1}^m \mathbb{A}.term[i][k] * \mathbb{B}.term[k][j];\\
%&\forall_{\substack{i \in \{1, \dots, n\}\\ j \in \{1, \dots, m\} }}\text{ }\mathbb{C}.term[i][j] := \sum_{k = 1}^m \mathbb{A}.term[i][k] * \mathbb{B}.term[k][j];\\
\end{align*}
\caption{Grammar rules operating on two expressions.}
\label{fig:rules2}
\end{framed}
\end{figure}

\begin{figure}
\begin{framed}
\begin{align*}
&\text{{\bf Columns marginalization}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{n \times 1}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}.n * \mathbb{A}.m);\\
&\mathbb{C}.computation := sum(\mathbb{A}.computation, 2); \\
&\forall_{i \leq n}\text{ }\mathbb{C}.term[i][1] := \sum_{j = 1}^m \mathbb{A}.term[i][j];\\
& \\
&\text{{\bf Rows marginalization}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{1 \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}.n * \mathbb{A}.m);\\
&\mathbb{C}.computation := sum(\mathbb{A}.computation, 1); \\
&\forall_{j \leq m}\text{ }\mathbb{C}.term[1][j] := \sum_{i = 1}^n \mathbb{A}.term[i][j];\\
& \\
&\text{{\bf Columns repetition}}\\
&\mathbb{A} \in \mathcal{P}^{n \times 1}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}.n * m);\\
&\mathbb{C}.computation := repmat(\mathbb{A}.computation, 1, m); \\
&\forall_{\substack{i \leq n\\ j \leq  m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[1][j];\\
& \\
&\text{{\bf Rows repetition}}\\
&\mathbb{A} \in \mathcal{P}^{1 \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + n * \mathbb{A}.m);\\
&\mathbb{C}.computation := repmat(\mathbb{A}.computation, n, 1); \\
&\forall_{\substack{i \leq n\\j \leq m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[i][1];\\
& \\
&\text{{\bf Entry repetition}}\\
&\mathbb{A} \in \mathcal{P}^{1 \times 1}_\alpha, t_\mathcal{A} \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + n * m);\\
&\mathbb{C}.computation := repmat(\mathbb{A}.computation, n, m); \\
&\forall_{\substack{i \leq n\\ j \leq m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[1][1];\\
& \\
&\text{{\bf Transposition}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{m \times n}_\alpha\\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}n * \mathbb{A}.m);\\
&\mathbb{C}.computation := \mathbb{A}.computation';\\
&\forall_{\substack{i \leq n\\j \leq m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[j][i];
\end{align*}
\caption{Grammar rules operating on one expression.}
\label{fig:rules}
%\caption{Set of rules for attribut grammar. There is always finite number of literals up to degree $\alpha$.}
\end{framed}
\end{figure}


To find any (or the cheapest) way of computing target expression $\mathbb{T}$ of degree $k$, we proceed as follows: 
\begin{itemize}
\item Develop the grammar to obtain all possible expressions up to degree
  $k$. It gives rise to a finite number of expressions. 
\item If there are multiple ways of computing the same expression up
  to a multiplicative constant, choose one with shortest time.
\item Find a linear combination of expressions which equals the target
  expression $\mathbb{T}$ (this relation is a symbolic relation, which
  is valid for any assignment of symbols). While solving this linear
  system, choose the solution with the smallest total computation
  time. See \ref{sec:linear} for more details.
\item Apply optimization like sub-expression elimination, and exploit
  the distributive property of multiplication in order to decrease the
  final computational time of $\mathbb{T}$. This step is optional and
  won't decrease the computational complexity, but can improve
  performance.
\end{itemize}

Pseudo code is shown in Algorithm \ref{pseudocode}.

\begin{algorithm}[t]
\caption{Find computation for expression $\mathbb{T}$}
\begin{algorithmic} 
\REQUIRE Target expression $\mathbb{T}$, initial expression $W$, $Rules$ = \{$R_1, \dots, R_n$\}, maximum degree of polynomials $k$
\ENSURE Computation $\mathcal{C}$ of expression $\mathbb{T}$.
\STATE Initialize set $\mathbb{S}$ of all admissible expressions with $W$
\WHILE{$\mathbb{S}$ grows}
\FORALL{rule $R \in Rules$}
\STATE $\mathbb{S}^2 = \{ (x, y) : x \in \mathbb{S} \land y \in \mathbb{S}\}$ 
\FORALL{Expression $E \in \mathbb{S}\cup\mathbb{S}^2$}
\IF {can't apply $R$ to $E$}
\STATE \textbf{continue}
\ENDIF
\STATE $E' \gets$ Apply rule $R$ to $E$
\IF {degree($E'$) $>$ $k$}
  \STATE \textbf{continue}
\ENDIF
\STATE \emph{// If expression not yet in $\mathbb{S}$ or it can be}
\STATE \emph{// computed faster, then add it.}
\IF {$E'.expr \not\in \mathbb{S}$ \textbf{or} $E'.time < S[E'].time$}
  \STATE Add $E'$ to $\mathbb{S}$
\ENDIF
\ENDFOR
\ENDFOR
\ENDWHILE
\STATE Find $\mathcal{C}$, the linear combination of expressions stored in $\mathbb{S}$ to express $\mathbb{T}$
\STATE Run optimizer on $\mathcal{C}$ (optional)
\end{algorithmic}
\label{pseudocode}
\end{algorithm}


\subsection{Linear combination}
\label{sec:linear}

We search for the linear combination of generated expressions which equal the target expression. 
There are several possible scenarios:
\begin{itemize}
  \item \emph{single solution} - linear solver will have a unique solution (in practice
 this is uncommon).
 \item \emph{no solution} - target expression is out of scope for the computation defined by
our grammar. 
 \item  \emph{multiple solutions} - we look for the linear combination with the smallest cost
(computational time). 
\end{itemize}
The multiple solutions case is essentially an integer programing
problem, or knapsack problem, which is NP-complete. However, we can
relax this to a linear program, which will give a solution almost as
efficient as the optimal one. Experiments suggest that it is good to
also minimize number of coefficients, so avoiding unnecessary
non-integer coefficient values, which can contribute numerical errors.


It is worth noting that while it is easy to find best solution in
terms of {\em complexity}, it is NP-complete to find the best solution
in terms of {\em performance}. In order to find best solution in terms
of complexity, one has to run linear solver multiple times. Every
time, it should include broader number of expressions, starting with set
of expressions of $O(1)$ complexity and adding new expressions with higher and higher complexity
($O(n)$ and then $O(n^2)$ and so on). If algorithm finds a solution at
some point, then it is optimal in terms of complexity. 
However, discovery of the fastest performance (i.e.~lowest
run-time) requires exploration of all possible combinations of expressions,
which is far more expensive.  

\vspace{-2mm}
\subsection{Computation derivation as the algebra}
The space of matrices of polynomials forms an algebra $\mathcal{A}$.
A \emph{grammar} $\mathcal{G}=\{g_1,\dots,g_{K};\, h_1,\dots,
h_{K'}\}$ defined on this algebra is a set of functions:
\vspace{-1mm}
\begin{eqnarray}
g_i &:& \mathcal{A} \longrightarrow \mathcal{A}~,~(i \leq
K)~,\nonumber \\
h_{i} &:& \mathcal{A} \times \mathcal{A} \longrightarrow \mathcal{A}~,
~i \leq K'~,\nonumber 
 \vspace{-2mm} 
\end{eqnarray}
which might eventually only defined for certain sub-algebras of $\mathcal{A}$ 
(e.g.~matrix multiplication requires inner dimensions to agree). 

Given an initial element $w \in \mathcal{A}$ and a target element $t \in \mathcal{A}$, 
the question is whether $t$ can be written as a finite sequence of operations involving 
only $w$ and $g,h \in \mathcal{G}$.
If the algebra is defined over a discrete field, then the problem can be  
cast as a shortest path problem on a sparse directed graph. 
We define a ``sink" node which contains all polynomials (or matrices of polynomials) with degree $\geq D_{\max}$ 
where ($D_{\max} > \mbox{deg}(t)$), and a node for each of the other elements of $\mathcal{A}$. 
Two nodes $x_1,\,x_2$ have a connection $x_1 \rightarrow x_2$ if there exists $g \in \mathcal{G}$ such that
$x_2 = g(x_1)$. The edge is assigned a cost which depends on the complexity of performing the operation.   
Similarly, we can define a 3-cycle to $(x_1,x_2,x_3)$ if there exists
$h \in \mathcal{G}$ such that $x_3 = h(x_1,x_2)$. The brute force algorithm constructs the graph by advancing sequentially from smallest to largest degree. 


\subsection{Representation of Expressions} \label{representation}
%Rob: I did not understand this entire section.
\ref{itm:expression}s belong to $\mathcal{P}^{n \times m}_\alpha$. There are various
ways how they can be represented in software and the choice is
important as it has consequences in the correctness of the solution
(due to numerical errors), as well as speed of computation.

Let us consider two matrices of polynomials:
\begin{equation}
\label{eq:a_matrix}
%\begin{align*}
\mathbb{A} = \begin{pmatrix} a^2 + 2ab + b^2 & a^2\\ b^2 & ab \end{pmatrix} 
%\end{align*}
\end{equation}
\begin{equation}
\label{eq:b_matrix}
\mathbb{B} = \begin{pmatrix} a + b & a\\ b & 2a + 2b \end{pmatrix} \vspace{-1mm}
\vspace{-1mm}
\end{equation}
We consider element-wise matrix multiplication of $\mathbb{A}$ and
$\mathbb{B}$. We derive the result using three different ways of
representating expressions.



\subsubsection{Symbolic representation}
The most direct representation of expressions is to store its coefficients and powers of the every monomial.
This representation is exact, and corresponds one-to-one with our description of operations
defined by grammar. It guarantees correctness and is easy to debug. 
However, it has computational drawback: multiplying two polynomials, each having $N$ monomials is $O(N^2)$ compared to the same operation on instantiations of this polynomials, which is $O(1)$.
The symbolic representation allowed us
to find patterns for polynomials up to degree $4$ in $\sim 8$ hours of computation on a standard laptop. 
The example of this representations for matrices \ref{eq:a_matrix} and \ref{eq:b_matrix} can be found
in Table \ref{representation_examples}.

\subsubsection{Evaluation of polynomials in $\mathbb{R}$}
Polynomials can be encoded by their evaluation at various, random points.
This encoding simplifies operations on expressions, and the time to multiply expressions
is proportional to the number of evaluation points (for every evaluation
point, we just have to perform floating point multiplication).

We need to evaluate our polynomials at a larger number of points than
the final size of the linear system of equations (see Section
\ref{sec:linear}). Note that we do not have to recover the
coefficients of polynomials, as we are just interested in finding the
proper linear combination of expressions.  Unfortunately, this method
is numerically unstable, and for polynomials of degree $k \geq 3$ it
is unable to discover solutions, even if they exist.

\subsubsection{Evaluation of polynomials in $\mathbb{Z}_p$}
We can avoid numerical issues by evaluating polynomials in the
$\mathbb{Z_p}$ group (for some large prime $p$), instead of
$\mathbb{R}$. This guarantees all the results of the computation
to be exact since there are no rounding errors on integers, and values are
bounded by $p$.

Although this representation is the fastest, and has no problems with
numerical errors, it is difficult to implement and debug. Using this
representation, we were able to completely develop grammars up to 
degree $6$. Table \ref{representation_examples} shows the example
of Eqns.~\ref{eq:a_matrix} \& \ref{eq:b_matrix} for $a = 1, b = 2$ and $p = 11$.

\begin{table}
\tiny
\centering
\begin{tabular}{llll}
\hline
Representation & $\mathbb{A}$ & $\mathbb{B}$ & $\mathbb{A}$ .* $\mathbb{B}$ \\
\hline
\specialcell{Symbolic\\(cell 1,1)} & \specialcell{$\begin{pmatrix} 2 & 1 & 0\\ 0 & 1 & 2 \end{pmatrix}$\\$\alpha = [1, 2, 1]$}  & \specialcell{$\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$\\$\alpha = [1, 1]$} & \specialcell{$\begin{pmatrix} 3 & 2 & 1 & 0 \\ 0 & 1 & 2 & 3 \end{pmatrix}$\\$\alpha = [1, 3, 3, 1]$} \\[15pt]

$\mathbb{R}$ & $\begin{pmatrix} 4 & 0.25\\ 2.25 & 0.75 \end{pmatrix}$ & $\begin{pmatrix} 2 & 0.5\\ 1.5 & 4 \end{pmatrix}$ & $\begin{pmatrix} 8 & 0.125\\ 3.375 & 3 \end{pmatrix}$ \\[15pt]

$\mathbb{Z}_p$ ($p=11$) & $\begin{pmatrix} 9 & 1\\ 4 & 2 \end{pmatrix}$ & $\begin{pmatrix} 3 & 1\\ 2 & 6 \end{pmatrix}$ & $\begin{pmatrix} 5 & 1\\ 8 & 1 \end{pmatrix}$ \\
\hline
\end{tabular}
\caption{Comparision of representation types described in section \ref{representation}. The table contains instantiation of matrices \ref{eq:a_matrix} and \ref{eq:b_matrix}, for $a=1, b=2$ (symbolic and $\mathbb{Z}_p$) or $a=0.5, b=1.5$ ($\mathbb{R}$) representations. For symbolic, we present only the representation of the top-left (1,1) matrix cell and the letter $\alpha$ denotes the vector of coefficients.} 
\label{representation_examples}
\end{table}

