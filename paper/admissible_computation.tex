\section{Admissible Computation as a Grammar}\label{sec:grammars}

Our goal is to find a fast way to compute the target expression in
language and architecture of our choice. We will achieve it by
developing literals in a grammar, up to degree of target
expression. Next, we will look for the linear combination of obtained
expressions to find how to express target expression exactly. If there
are multiple literals with the same expression (up to multiplicative real
constant), we choose one with the shortest computation time $t_\mathcal{C}$.

Let's ground our approach in an example. We assume that we are interested in
finding algorithm with smallest operation complexity, and computation platform
is a Matlab. We consider following set of operations on matrices: 


\begin{figure}
\begin{framed}
\begin{align*}
&\text{{\bf Element wise multiplication}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathbb{B} \in \mathcal{P}^{n \times m}_\beta \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_{\alpha + \beta} \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{B}.time + \mathbb{A}.n * \mathbb{B}.m);\\
&\mathbb{C}.computation := \mathbb{A}.computation .* \mathbb{B}.computation;\\
&\forall_{\substack{i \leq n\\j \leq m}}\text{ }\mathbb{C}.term[i][j] := \mathbb{A}.term[i][j] * \mathbb{B}.term[i][j];\\
%&\forall_{\substack{1 \leq i \leq n\\1 \leq j \leq m}}\text{ }\mathbb{C}.term[i][j] := \mathbb{A}.term[i][j] * \mathbb{B}.term[i][j];\\
& \\
&\text{{\bf Matrix multiplication}}\\
&\mathbb{A} \in \mathcal{P}^{n \times k}_\alpha, \mathbb{B} \in \mathcal{P}^{k \times m}_\beta \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_{\alpha + \beta} \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{B}.time + \mathbb{A}.n * \mathbb{A}.m * \mathbb{B}.m);\\
&\mathbb{C}.computation := \mathbb{A}.computation * \mathbb{B}.computation;\\
&\forall_{\substack{i \leq n\\j \leq m}}\mathbb{C}.term[i][j] := \sum_{k = 1}^m \mathbb{A}.term[i][k] * \mathbb{B}.term[k][j];
%&\forall_{\substack{1 \leq i \leq n\\1 \leq j \leq m}}\mathbb{C}.term[i][j] := \sum_{k = 1}^m \mathbb{A}.term[i][k] * \mathbb{B}.term[k][j];\\
%&\forall_{\substack{i \in \{1, \dots, n\}\\ j \in \{1, \dots, m\} }}\text{ }\mathbb{C}.term[i][j] := \sum_{k = 1}^m \mathbb{A}.term[i][k] * \mathbb{B}.term[k][j];\\
\end{align*}
\caption{Grammar rules operating on two expressions.}
\label{fig:rules2}
\end{framed}
\end{figure}

\begin{figure}
\begin{framed}
\begin{align*}
&\text{{\bf Columns marginalization}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{n \times 1}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}.n * \mathbb{A}.m);\\
&\mathbb{C}.computation := sum(\mathbb{A}.computation, 2); \\
&\forall_{i \leq n}\text{ }\mathbb{C}.term[i][1] := \sum_{j = 1}^m \mathbb{A}.term[i][j];\\
& \\
&\text{{\bf Rows marginalization}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{1 \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}.n * \mathbb{A}.m);\\
&\mathbb{C}.computation := sum(\mathbb{A}.computation, 1); \\
&\forall_{j \leq m}\text{ }\mathbb{C}.term[1][j] := \sum_{i = 1}^n \mathbb{A}.term[i][j];\\
& \\
&\text{{\bf Columns repetition}}\\
&\mathbb{A} \in \mathcal{P}^{n \times 1}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}.n * m);\\
&\mathbb{C}.computation := repmat(\mathbb{A}.computation, 1, m); \\
&\forall_{\substack{i \leq n\\ j \leq  m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[1][j];\\
& \\
&\text{{\bf Rows repetition}}\\
&\mathbb{A} \in \mathcal{P}^{1 \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + n * \mathbb{A}.m);\\
&\mathbb{C}.computation := repmat(\mathbb{A}.computation, n, 1); \\
&\forall_{\substack{i \leq n\\j \leq m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[i][1];\\
& \\
&\text{{\bf Entry repetition}}\\
&\mathbb{A} \in \mathcal{P}^{1 \times 1}_\alpha, t_\mathcal{A} \rightarrow \mathbb{C} \in \mathcal{P}^{n \times m}_\alpha \\
&\mathbb{C}.time := O(\mathbb{A}.time + n * m);\\
&\mathbb{C}.computation := repmat(\mathbb{A}.computation, n, m); \\
&\forall_{\substack{i \leq n\\ j \leq m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[1][1];\\
& \\
&\text{{\bf Transposition}}\\
&\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha \rightarrow \mathbb{C} \in \mathcal{P}^{m \times n}_\alpha\\
&\mathbb{C}.time := O(\mathbb{A}.time + \mathbb{A}n * \mathbb{A}.m);\\
&\mathbb{C}.computation := \mathbb{A}.computation';\\
&\forall_{\substack{i \leq n\\j \leq m}}\text{ } \mathbb{C}.term[i][j] := \mathbb{A}.term[j][i];
\end{align*}
\caption{Grammar rules operating on one expression.}
\label{fig:rules}
%\caption{Set of rules for attributive grammar. There is always finite number of literals up to degree $\alpha$.}
\end{framed}
\end{figure}


To find any (or the cheapest) way of computing target expression $\mathbb{T}$ of degree $k$, we proceed as follows: 
\begin{itemize}
\item Develop the grammar to obtain all possible literals up to degree
  $k$. It gives rise to a finite number of literals. 
\item If there are multiple ways of computing the same expression up
  to a multiplicative constant, choose one with shortest time.
\item Find a linear combination of expressions which equals the target
  expression $\mathbb{T}$ (this relation is a symbolic relation, which
  is valid for any assignment of symbols). While solving this linear
  system, choose the solution with the smallest total computation
  time. See \ref{sec:linear} for more details.
\item Apply optimization like sub-expression elimination, and exploit
  the distributive property of multiplication in order to decrease the
  final computational time of $\mathbb{T}$. This step is optional, and
  won't decrease computational complexity, but can improve
  performance.
\end{itemize}

Pseudo code for the algorithm is shown in Algorithm \ref{pseudocode}.

\begin{algorithm}[t]
\caption{Find computation for expression $\mathbb{T}$}
\begin{algorithmic} 
\REQUIRE Target expression $\mathbb{T}$, initial expression $W$, $Rules$ = \{$R_1, \dots, R_n$\}, maximum degree of polynomials $k$
\ENSURE Computation $\mathcal{C}$ of expression $\mathbb{T}$.
\STATE Initialize set $\mathbb{S}$ of all admissible literals with $(W, \mathcal{W}, t_W)$
\WHILE{$\mathbb{S}$ grows}
\FORALL{rule $R \in Rules$}
\STATE $\mathbb{S}^2 = \{ (x, y) : x \in \mathbb{S} \land y \in \mathbb{S}\}$ 
\FORALL{literal $L \in \mathbb{S}\cup\mathbb{S}^2$}
\IF {can't apply $R$ to $L$}
\STATE \textbf{continue}
\ENDIF
\STATE $L' \gets$ Apply rule $R$ to $L$
\IF {degree($L'$) $>$ $k$}
  \STATE \textbf{continue}
\ENDIF
\STATE \emph{// If expression not yet in $\mathbb{S}$ or it can be}
\STATE \emph{// computed faster, then add it.}
\IF {$L'.expr \not\in \mathbb{S}$ \textbf{or} $L'.time < S[L'].time$}
  \STATE Add $L'$ to $\mathbb{S}$
\ENDIF
\ENDFOR
\ENDFOR
\ENDWHILE
\STATE Find $\mathcal{C}$, the linear combination of expressions from literals stored in $\mathbb{S}$ to express $\mathbb{T}$
\STATE Run optimizer on $\mathcal{C}$ (optional)
\end{algorithmic}
\label{pseudocode}
\end{algorithm}


\subsection{Linear combination}
\label{sec:linear}

We look for a linear combination of generated expressions in our
literals, which is equal to the target expression. There are several
possible scenarios:
\begin{itemize}
  \item \emph{single solution} - linear solver will have a unique solution (in practice
 this is uncommon)
 \item \emph{no solution} - target expression is out of scope for the computation defined by
our grammar. 
 \item  \emph{multiple solutions} - we look for the linear combination with the smallest cost
(computational time). 
\end{itemize}

Multiple solutions case is essentially an integer programing
problem, or knapsack problem, which are NP-complete. However, we can
relax this to a linear program, which will give a solution almost as
efficient as the optimal one. Experiments suggest that is good to
also minimize number of coefficients, so avoiding unnecessary
non-integer coefficient values, which can contribute numerical errors.


It is worth noting that while it is easy to find best solution in
terms of {\em complexity}, it is NP-complete to find the best solution
in terms of {\em performance}. In order to find best solution in terms
of complexity, one has to run linear solver multiple times. Every
time, it should include broader number of literals. Starting with set
of literals having the smallest complexity (e.g.~those which are
$O(1)$), and adding new literals with higher and higher complexity
($O(n)$ and then $O(n^2)$ and so on). If algorithm, find solution at
some point, than this solution is optimal in terms of
complexity. However, discovery of the fastest performance (i.e.~lowest
run-time) requires exploration of all possible combinations of literals,
which is far more expensive.  

\subsection{Computation derivation as the algebra}
The space of matrices of polynomials forms an algebra $\mathcal{A}$.
A \emph{grammar} $\mathcal{G}=\{g_1,\dots,g_{K};\, h_1,\dots, h_{K'}\}$ defined on this algebra is a set of functions
$$g_i : \mathcal{A} \longrightarrow \mathcal{A}~,~(i \leq K)~,$$
$$h_{i} : \mathcal{A} \times \mathcal{A} \longrightarrow \mathcal{A}~, ~i \leq K'~,$$
which might eventually only defined for certain sub-algebras of $\mathcal{A}$ 
(eg matrix multiplication requires inner dimensions to agree). 

Given an initial element $w \in \mathcal{A}$ and a target element $t \in \mathcal{A}$, 
the question is whether $t$ can be written as a finite sequence of operations involving 
only $w$ and $g,h \in \mathcal{G}$.
If the algebra is defined over a discrete field, then the problem can be  
cast as a shortest path problem on a sparse directed graph. 
We define a ``sink" node which contains all polynomials (or matrices of polynomials) with degree $\geq D_{\max}$ 
($D_{\max} > \mbox{deg}(t)$ ), and a node for each of the other elements of $\mathcal{A}$. 
Two nodes $x_1,\,x_2$ have a connection $x_1 \rightarrow x_2$ if there exists $g \in \mathcal{G}$ such that
$x_2 = g(x_1)$. The edge is assigned a cost which depends on the complexity of performing the operation.   
Similarly, we can define a 3-cycle to $(x_1,x_2,x_3)$ if there exists $h \in \mathcal{G}$ such that $x_3 = h(x_1,x_2)$.

The brute force algorithm constructs the graph by advancing sequentially from smallest to largest degree. 


\subsection{Representation of Expressions}
%Rob: I did not understand this entire section.
\ref{itm:expression}s belong to $\mathcal{P}^{n \times m}_\alpha$. There are various
ways how they can be represented in the software. Such representation is crucial, and has consequences in
correctness of solution (due to numerical errors), and computation speed.

Let us consider two matrices of polynomials:
\begin{align*}
\mathbb{A} &= \begin{pmatrix} a^2 + 2ab + b^2 & a^2\\ b^2 & ab \end{pmatrix}, \mathbb{B} &= \begin{pmatrix} a + b & a\\ b & 2a + 2b \end{pmatrix}
\end{align*}
We consider element-wise matrix multiplication of $\mathbb{A}$ and $\mathbb{B}$. We derive result for different representation of expressions.



\subsubsection{Symbolic representation}
The most direct representation of expressions is to store its coefficients and powers of the every monomial.
This representation is exact, and corresponds one-to-one with our description of operations
defined by grammar. However, it has computational drawback. 
Let's assume that we would like to multiply two polynomials, each having $1000$ monomials.
Such multiplication would take roughly $10^6$ operations. While the same operation on instantiations of
this polynomials would cost just a one multiplication.


We have implemented expressions in such representation, and it allowed us
to find patterns for polynomials up to degree $4$ in $\sim 8$ hours of computation. We haven't run computation for 
higher degrees due to resource limitations. This representation guarantees correctness, and is easy to debug, however
is quite slow (as the consequence of verboseness). 

Let's ground this representation in an example of element-wise matrix multiplication of $\mathbb{A}$, and $\mathbb{B}$.
Result of such multiplication is a matrix. We present results for the entry $(0, 0)$.
Entry $(0, 0)$ of the matrix $\mathbb{A}$ is the list of coefficients $[1, 2, 1]$, and the list
of powers $\begin{pmatrix} 2 & 1 & 0\\ 0 & 1 & 2 \end{pmatrix}$. Entry $(0, 0)$ of the matrix $\mathbb{B}$ is
the list of coefficients $[1, 1]$, and the list of powers 
$\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$. Entry $(0, 0)$ of the element-wise multiplication of $\mathbb{A}$, and $\mathbb{B}$
is the list of coefficients $[1, 3, 3, 1]$, and
the list of powers $\begin{pmatrix} 3 & 2 & 1 & 0 \\ 0 & 1 & 2 & 3 \end{pmatrix}$. 


\subsubsection{Evaluation of polynomials in $\mathbb{R}$}
Polynomials can be encoded by its evaluation at various, random points.
This encoding simplifies operations on expressions, and multiplication of expressions
in such representation is proportional to the number of evaluation points (for every evaluation
point, we have to perform just a common multiplication).

We need evaluate our polynomials in number of points which is larger 
than final size of linear equations \ref{sec:linear}. We do not have to recover coefficients of polynomials,
as we are just interest in finding proper linear combination of expressions.

This methods is numerically unstable, and for polynomials of degree $k \geq 3$, 
it is not able to produce solutions, even if there are some.

Let us ground this examples. We consider 2 points were we evaluate our polynomials:
$a = 0.5, b = 1.5$, and $a = 2.5, b = 3.5$.

Then $\mathbb{A}, \mathbb{B}$ have the following representation:
\begin{align*}
\mathbb{A} &= \begin{pmatrix} 4 & 0.25\\ 2.25 & 0.75 \end{pmatrix}, \begin{pmatrix} 36 & 6.25\\ 12.25 & 8.75 \end{pmatrix} \\
\mathbb{B} &= \begin{pmatrix} 2 & 0.5\\ 1.5 & 4 \end{pmatrix}, \begin{pmatrix} 6 & 2.5\\ 3.5 & 12 \end{pmatrix}
\end{align*}
Element-wise multiplication of expressions in this representation $\mathbb{A}$, and $\mathbb{B}$ is just element-wise multiplication of evaluations
in the same points:
\begin{align*}
\begin{pmatrix} 8 & 0.125\\ 3.375 & 4 \end{pmatrix}, \begin{pmatrix} 216 & 15.625\\ 42.875 & 105 \end{pmatrix}
\end{align*}
This representation is numerically unstable (we are getting a lot of round offs, and large values).

\subsubsection{Evaluation of polynomials in $\mathbb{Z}_p$}
We use similar strategy as in case of evaluations of polynomials in $\mathbb{R}$. 
We avoid numerical errors by replacing computation on real number with computation in $\mathbb{Z}_p$ group (for a large prime number $p$). This
guarantees that all the results of the computation are exact (there is no round off errors on integers, and values are bounded by $p$). 

This representation is fastest, and has no problem with numerical errors. However, it is difficult to debug. With such representation, we were able to
develop completely grammars up to the degree $5$.


Let's assume that we evaluate polynomials in points $a = 1, b = 2$, and $a = 3, b = 4$, and we work in $Z_{11}$.
Then $\mathbb{A}, \mathbb{B}$ have the following representation:
\begin{align*}
\mathbb{A} &= \begin{pmatrix} 9 & 1\\ 4 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 9\\ 5 & 1 \end{pmatrix} \\
\mathbb{B} &= \begin{pmatrix} 3 & 1\\ 2 & 6 \end{pmatrix}, \begin{pmatrix} 7 & 3\\ 4 & 3 \end{pmatrix}
\end{align*}
Element-wise multiplication of expressions $\mathbb{A}$, and $\mathbb{B}$ is just element-wise multiplication of evaluations
in the same points modulo $11$:
\begin{align*}
\begin{pmatrix} 5 & 1\\ 8 & 1 \end{pmatrix}, \begin{pmatrix} 2 & 5\\ 9 & 3 \end{pmatrix}
\end{align*}

