Our goal is to find a fast way to compute the target expression in
language and architecture of our choice.  We will achieve it by
developing literals in a grammar, up to degree of target
expression. Next, we will look for the linear combination of obtained
expressions to find how to express target expression exactly. If there
are multiple literals with the same expression (up to multiplicative real
constant), we choose one with the shortest computation time $t_\mathcal{C}$.

Let's ground our approach in an example. We assume that we are interest in
finding algorithm with smallest operation complexity, and computation platform
is a Matlab.  We consider following set of operations on matrices: 

\begin{figure}
\begin{framed}
\begin{align*}
&\text{{\bf Element wise multiplication}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}), (\mathbb{B} \in \mathcal{P}^{n \times m}_\beta, \mathcal{B}, t_\mathcal{B}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_{\alpha + \beta}, \mathcal{A} .* \mathcal{B}, O(t_\mathcal{A} + t_\mathcal{B} + nm)) \\
&\mathbb{C}_{i,j} = \mathbb{A}_{i,j}\mathbb{B}_{i, j} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\} \\
&\text{{\bf Matrix multiplication}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}), (\mathbb{B} \in \mathcal{P}^{m \times k}_\beta, \mathcal{B}, t_\mathcal{B}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times k}_{\alpha + \beta}, \mathcal{A} * \mathcal{B}, O(t_\mathcal{A} + t_\mathcal{B} + nmk)) \\
&\mathbb{C}_{i,k} = \sum_{k = 1}^m \mathbb{A}_{i,k}b_{k, j} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\} \\
&\text{{\bf Columns marginalization}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times 1}_\alpha, sum(\mathcal{A}, 2), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, 1} = \sum_{j = 1}^m \mathbb{A}_{i, j} \text{ for } i \in \{1, \dots, n\}\\
&\text{{\bf Rows marginalization}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{1 \times m}_\alpha, sum(\mathcal{A}, 1), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{1, j} = \sum_{i = 1}^m \mathbb{A}_{i, j} \text{ for } j \in \{1, \dots, m\}\\
&\text{{\bf Columns repetition}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times 1}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_\alpha, repmat(\mathcal{A}, 1, m), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, j} = \mathbb{A}_{i, 1} \text{ for } i \in \{1, \dots, n\}\\
&\text{{\bf Rows repetition}}\\
&(\mathbb{A} \in \mathcal{P}^{1 \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_\alpha, repmat(\mathcal{A}, n, 1), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, j} = \mathbb{A}_{1, j} \text{ for } j \in \{1, \dots, m\}\\
&\text{{\bf Entry repetition}}\\
&(\mathbb{A} \in \mathcal{P}^{1 \times 1}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{n \times m}_\alpha, repmat(\mathcal{A}, n, m), O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{i, j} = \mathbb{A}_{1, 1} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\}\\
&\text{{\bf Transposition}}\\
&(\mathbb{A} \in \mathcal{P}^{n \times m}_\alpha, \mathcal{A}, t_\mathcal{A}) \rightarrow \\ 
&(\mathbb{C} \in \mathcal{P}^{m \times n}_\alpha, A', O(t_\mathcal{A} + nm)) \\
&\mathbb{C}_{j, i} = \mathbb{A}_{i, j} \text{ for } i \in \{1, \dots, n\}, j \in \{1, \dots, m\} \\
\end{align*}
\caption{Set of rules for attributive grammar. There is always finite number of literals up to degree $\alpha$.}
\end{framed}
\label{rules}
\end{figure}


To find any (or the cheapest) way of computing target expression $\mathbb{T}$ of degree $k$, we proceed as follows: 
\begin{itemize}
\item develop grammar to obtain all possible literals up to degree $k$. It gives rise to finite number of literals. 
\item if there are multiple way of computing the same expression up to multiplicative constant, choose one with shortest time.
\item find linear combination of expressions which is equal the target
  expression $\mathbb{T}$ (this relation is a symbolic relation, which
  is valid for any assignment of symbols). While solving this linear
  set of systems, choose solution with smallest sum of computation
  times. See \ref{sec:linear} for more details.
\item apply optimization like subexpression elimination, and exploit distributive property of multiplication in order to decrees final computational time of $\mathbb{T}$. This step is optional, and it won't decrease computational complexity, but it can improve performance.
\end{itemize}

Pseudo code for the algorithm is shown in Algorithm \ref{pseudocode}.

\begin{algorithm}
\caption{Find computation for expression}
\begin{algorithmic} 
\REQUIRE Target expression $\mathbb{T}$, initial expression $W$, $Rules$ = \{$R_1, \dots, R_n$\}, maximum degree of polynomials $k$
\ENSURE Computation $\mathcal{C}$ of expression $\mathbb{T}$.
\STATE Initialize set $\mathbb{S}$ of all admissible literals with $(W, \mathcal{W}, t_W)$
\WHILE{$\mathbb{S}$ grows}
\FORALL{literal $L \in \mathbb{S}$}
\FORALL{rule $R \in Rules$}
\STATE $L' \gets$ Apply rule $R$ to $L$
\IF {degree($L'$) $>$ $k$}
  \STATE \textbf{continue}
\ENDIF
\STATE \emph{// If expression not yet in $\mathbb{S}$ or it can be}
\STATE \emph{// computed faster add it.}
\IF {$L'.expr \not\in \mathbb{S}$ \textbf{or} $L'.time < S[L'].time$}
  \STATE Add $L'$ to $\mathbb{S}$
\ENDIF
\ENDFOR
\ENDFOR
\ENDWHILE
\STATE Find linear combination of expressions from literals stored in $\mathbb{S}$ to express $\mathbb{T}$
\STATE Run optimizer (optional)
\end{algorithmic}
\label{pseudocode}
\end{algorithm}


\subsection{Linear combination}
\label{sec:linear}
We look for a linear combination of generated expressions in our
literals, which is equal to the target expression. There are several
possible scenarios. First, there might be a \emph{single solution}, in
which case our linear solver will have a unique solution. In practice
however, this is uncommon. Second, there may be \emph{no solution} at
all.  In this case, our linear solver will indicate it, meaning that
the target expression is out of scope for the computation defined by
our grammar. Finally, there might be \emph{multiple solutions}. We
then look for the linear combination with the smallest cost
(computational time).  This is essentially an integer programing
problem, or knapsack problem, which are NP-complete. However, we can
relax this to a linear program, which will give a solution almost as
efficient as the optimal one.  Experiments suggest that is good to
also minimize number of coefficients, so avoiding uneccessary
non-integer coefficient values, which can contribute numerical errors.


%it is enough for us to get approximated
%solution (Rob: what kind of approximation), which can be easily obtained with linear solver.

It is worth noting that while it is easy to find best solution in
terms of {\em complexity}, it is NP-complete to find the best solution
in terms of {\em performance}. In order to find best solution in terms
of complexity, one has to run linear solver multiple times. Every
time, it should include broader number of literals. Starting with set
of literals having the smallest complexity (e.g.~those which are
$O(1)$), and adding new literals with higher and higher complexity
($O(n)$ and then $O(n^2)$ and so on). If algorithm, find solution at
some point, than this solution is optimal in terms of
complexity. However, discovery of the fastest performance (i.e.~lowest
runtime) requires exporation of all possible combinations of literals,
which is far more expensive.  


% This section is not necessary and it's confusing. 
%\subsection{Optimization}
%Rob: English from here on not yet fixed
%
%The application of grammar rules gives rise to a tree. Moreover, last step of
%combining elements together can be also considered as adding a plus node to the
%tree. Optimization procedures (Rob: what kind?) are applied recursively to tree branches, and
%transform them to more efficient computationally equivalents. We used common in
%compiler literature optimization techniques like subexpression elimination.
%Further, we optimized our symbolic tree by exploiting (1) distributive, and (2)
%associative properties of multiplication, and addition (together with
%commutative property for addition).

\subsection{Extension of Grammar}
The presented attributive grammar is an example of a grammar which is finite
and can solve a class of problems involving polymonials. However, one could extend it with 
other operations such as matrix inverse, or subvision of the matrcies.
The latter would allow the discovery of recurrences (e.g.~ discreet Fourier transform, or
Strassen algorithm for fast matrix multiplication).

\subsection{Computation derivation as the formal system}

For Joan.

\subsection{Representation of Expressions}
Rob: I did not understand this entire section.
Expressions are defined as belonging to $\mathcal{P}^{n \times m}_\alpha$. There are various
ways how they can be represented in software. This representation is crucial, and has consequences in
 correctness of solution (due to numerical errors), and computation speed.
Rob: sometimes subscript is $k$ sometimes $\alpha$, fix to be
consistent. 

\subsubsection{Evaluation of polynomials in $\mathbb{R}$}
As expressions correspond to the matrices of polynomials, one could just record their values
for many random numbers. If number of evaluations is large enough, then polynomials can be
recovered based on such values. Or even one could keep this representation over entire 
computation without recovering polynomials, but deriving final relation between polynomials (that's what we implemented).
It turns out, that this methods is numerically unstable
, and for polynomials of degree $k \geq 3$, it is not able to produce solutions, even if there are some.

Nice part about this representation is that operations like element-wise matrix multiplication, addition of matrices, etc.
correspond just to the same operations in our representation (of same computational cost). i.e. For element-wise multiplication of matrices of expressions, we
have to perform element-wise multiplication of real numbers (on multiple instances of random values assignments).

\subsubsection{Symbolic representation}
Expressions can represented as matrices of polynomials. Every expression has list of coefficients and monomials, which it constitutes of.
This representation is exact, and gives us guarantees on correctness. However,
it might be quite slow. For instance, if two polynomials, which we want to multiply constitutes of 1000 coefficients, then
it takes $\sim 1mln.$ operations. We have implemented expressions in such representation, and it allowed us
to find patterns for polynomials up to degree $4$ (above degree $4$ it was consuming too much time).

This solution guarantees correctness, and it is easiest to debug.

\subsubsection{Evaluation of polynomials in $\mathbb{Z}_p$}
We use similar strategy as in case of looking into evaluations of polynomials in $\mathbb{R}$. However, in order
to avoid numerical errors, we replace real number computation with computation in $\mathbb{Z}_p$ group (for large prime number $p$). This
guarantees that all the results of the computation are represented correctly (there is no round off errors on integers). Nonetheless, there are few potential
issues. We could have two different expressions being represented the same way (conflict of representations). In the
real setting, we haven't run into this problem. Moreover, all the final coefficients after applying linear solver are
accurate up to $p$ offset. This turns out also not to be an issue. For all coefficients above $\frac{p}{2}$, we subtract $p$. 
This way all final coefficients are in the range $[-\frac{p}{2}, \frac{p}{2}]$. We have verified that such solutions are correct
(it can be considered as prior on magnitude of coefficients). 

This solution is fastest, and most reliable. However, it is difficult to debug.

