\section{Dropout marginalization}\label{sec:dropout}
In this section we will perform analogous analisys to the one from Subsection \ref{subsubsec:gx} 
for a single layer network with $L_2$ loss. 
First, we derive update rules for weights for a single layer neural networks without any regularization. Then
we will confront it with dropout network. We will compute marginalization over
all possible dropout masks.
We will describe how our framework can discover
such update rules for more complex networks.

Single layer neural network with $L_2$ loss minimizes following objective:
\begin{equation*}
  \min_W ||WX - Y||^2, X \in \mathbb{R}^{f \times b}, Y \in \mathbb{R}^{g \times b}, W \in \mathbb{R}^{g \times f}
\end{equation*}
By differentiating above equation one gets gradient descent update rule
on $W$ (sign $\sim$ indicates that we drop multiplicative constants)
\begin{equation*}
 \partial W = 2(WX - Y)X^T \sim WXX^T - YX^T
\end{equation*}
We note for further derivation that $C = XX^T$ is the covariance matrix of
$X$, and that $C_{i,j} = \sum_k X_{i, k}X_{j, k}$.


Dropout minimizes following objective:
\begin{equation*}
  \argmin_W \mathbb{E}_M ||W(X .* M) - Y||^2, M \in \{0, 1\}^{f \times b}
\end{equation*}
Above expectation operator acts over all possible mask assignments, and $.*$ is 
element-wise multiplication. For simplification
let's assume that we drop neurons with probability $0.5$. Expectation
can be replaced with sum over all possible masks. Sd compute
derivative with respect to weights $W$. 
\begin{align*}
  \partial W \sim \sum_M (W(X .* M) - Y)(X .* M)^T = \\
  \sum_M W(X .* M)(X .* M)^T - Y(X .* M)^T = \\
  W\sum_M [(X .* M)(X .* M)^T] - 2^{fb - 1}YX^T= \\
\end{align*}
Let's denote $D = \sum_M (X .* M)(X .* M)^T$. 
\begin{align*}
  D_{i,j} = \sum_M \sum_k X_{i, k} X_{j, k} M_{i, k} M_{j, k} \\
  D_{i,j} \text{ for } i \neq j = 2^{fg - 2} \sum_k X_{i, k} X_{j, k} = 2^{fg - 2}C_{i, j} \\
  D_{i,j} \text{ for } i = j = 2^{fg - 1} \sum_k X_{i, k} X_{j, k} = 2^{fg - 1}C_{i, j} \\
  D = 2^{fg - 2}(XX^T + XX^T .* I) \text { $I$ is identity matrix } \\
  \partial W \sim \frac{1}{2}W (XX^T + XX^T .* I) - YX^T
\end{align*}

Above derivation of dropout marginalization can be discovered by our framework, and can be generalized
to multiple layer network with rational (quotient of two polynomials) activation functions. Our framework
has to be start with following start symbols: $W, X, Y, I$ (one needs additional symbol for identity).


Our framework is able to discover update weights $W$ for networks with more than one layer. 
For instance, it is able to find update rule for two-layer neural network objective:
\begin{equation*}
  \argmin_{W_1, W_2} \mathbb{E}_{M_1, M_2} ||W_2(f(W_1(X .* M_1)) .* M_2) - Y||^2
\end{equation*}
$f$ in this equation is a rational function. 


\section{Data augmentation marginalization}\label{sec:augm}
We discuss in this section how to marginalize augmentation of data translation, and change of contrast. 

\subsection{Data jittering}
For simplification we assume that our samples are one dimensional (instead of two dimensions as in
vision problems). Task is to find update rule, which integrates over all jitterings of data. This
corresponds to cropping inner part from the larger vector.

\begin{align*}
& \argmin_{W} \mathbb{E}_A \sum_{i = 0}^t||W(X_{i : (i + f)}) - Y||^2, X \in \mathbb{R}^{(f + t) \times b}\\
& \partial W \sim \sum_{i = 0}^t(W(X_{i : (i + f)}) - Y) X_{i : (i + f)}^T
\end{align*}

We denote by $T \in \{0, 1\}^{f \times (f + t)}$ matrix with $1$ up to $t$ values off the diagonal.
We obtain that:
\begin{align*}
\partial W \sim W(TXX^TT^T) - Y(TX)^T 
\end{align*}

To compute above expression, or one for more complex architectures, one
has to set $T, W, X, Y$ as starting symbols.

\subsection{Contrast alternation}
Task is to find update rule of weights, which integrates over all contrast alternations.
Let $A$ be a random variable describing how we want to alter contrast. Linear regression
with contrast alternation can be described as: 
\begin{align*}
& \argmin_{W} \mathbb{E}_A ||W(X + A) - Y||^2\\
\end{align*}
Usually contrast alternation is described with continuous distribution. It means that
expected value will be replaced with integral. Let's assume for simplicity that mean 
of contrast alternation is zero.
\begin{align*}
\partial W & \sim \int_A (W(X + A) - Y)(X + A)^T \\
   & = \int_A (W(XX^T + AA^T) - YX^T \\
   & = W(XX^T + \int AA^T) - YX^T 
\end{align*}
Update is modified by covariance matrix of contrast matrix. To discover computation,
which involves contrast alternation our framework would have to have access to 
expressions involving moments of $A$, and to starting symbols like $W, X, Y$.
Above derivation could be generalized to multilayer neural network. This would
give a single update rule consisting with respect to all possible alternated
input images.

